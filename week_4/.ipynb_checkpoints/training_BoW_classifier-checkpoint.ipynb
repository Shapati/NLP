{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUZyP90UjdHO"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Ali-Alameer/NLP/blob/main/week4_training_BoW_classifier.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "uFpAFvnZjdHP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# In this tutorial, we will be training a lot of models. In order to use GPU memory cautiously,\n",
        "# we will set tensorflow option to grow GPU memory allocation when required.\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices)>0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "dataset = tfds.load('ag_news_subset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIladSNmjdHQ"
      },
      "source": [
        "We can now access the training and test portions of the dataset by using `dataset['train']` and `dataset['test']` respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "OrNCM1zNjdHR"
      },
      "outputs": [],
      "source": [
        "ds_train = dataset['train']\n",
        "ds_test = dataset['test']\n",
        "\n",
        "print(f\"Length of train dataset = {len(ds_train)}\") # A formatted string literal or f-string is a string literal that is prefixed with 'f' or 'F'. These strings may contain replacement fields, which are expressions delimited by curly braces {}\n",
        "print(f\"Length of test dataset = {len(ds_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27usVyUjjdHR"
      },
      "source": [
        "Let's print out the first 10 new headlines from our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "7c_1xQD6jdHR"
      },
      "outputs": [],
      "source": [
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "\n",
        "for i,x in zip(range(10),ds_train):\n",
        "    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzbw2mGNjdHR"
      },
      "source": [
        "Another way to visualise the dataset is using \"take\" where we take a number of samples of the dataset as shown below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "5SK89gokjdHS"
      },
      "outputs": [],
      "source": [
        "list(ds_train.take(1).as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYrx2urijdHS"
      },
      "source": [
        "The adapt() method of keras ​\n",
        "Some preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:​\n",
        "\n",
        "TextVectorization: holds a mapping between string tokens and integer indices​\n",
        "\n",
        "Crucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data.​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "-DUF9ndcjdHU"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50000\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size)\n",
        "vectorizer.adapt(ds_train.take(500).map(lambda x: x['title']+' '+x['description']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "-ua1o-_vjdHV"
      },
      "outputs": [],
      "source": [
        "def to_bow(text):\n",
        "    return tf.reduce_sum(tf.one_hot(vectorizer(text),vocab_size),axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "0A1CIdLPjdHV"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sc_vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Beo9BQpjdHV"
      },
      "source": [
        "## Training the BoW classifier\n",
        "\n",
        "Now that we have learned how to build the bag-of-words representation of our text, let's train a classifier that uses it. First, we need to convert our dataset to a bag-of-words representation. This can be achieved by using `map` function in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "HKSDctyEjdHW"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "ds_train_bow = ds_train.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)\n",
        "ds_test_bow = ds_test.map(lambda x: (to_bow(x['title']+x['description']),x['label'])).batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxTFqyXZjdHW"
      },
      "source": [
        "Now let's define a simple classifier neural network that contains one linear layer. The input size is `vocab_size`, and the output size corresponds to the number of classes (4). Because we're solving a classification task, the final activation function is **softmax**:\n",
        "the units (4) is the dimensionality of the output space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "VA6z1u07jdHW"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(4,activation='softmax',input_shape=(vocab_size,))\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train_bow,validation_data=ds_test_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "NWFJFv_fjdHX"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1Vh5IS2jdHX"
      },
      "source": [
        "Since we have 4 classes, an accuracy of above 80% is a good result.\n",
        "\n",
        "## Training a classifier as one network\n",
        "\n",
        "Because the vectorizer is also a Keras layer, we can define a network that includes it, and train it end-to-end. This way we don't need to vectorize the dataset using `map`, we can just pass the original dataset to the input of the network.\n",
        "\n",
        "> **Note**: We would still have to apply maps to our dataset to convert fields from dictionaries (such as `title`, `description` and `label`) to tuples. However, when loading data from disk, we can build a dataset with the required structure in the first place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "fdKc0fiQjdHX"
      },
      "outputs": [],
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x),x['label'])\n",
        "\n",
        "inp = keras.Input(shape=(1,),dtype=tf.string)\n",
        "x = vectorizer(inp)\n",
        "x = tf.reduce_sum(tf.one_hot(x,vocab_size),axis=1)\n",
        "out = keras.layers.Dense(4,activation='softmax')(x)\n",
        "model = keras.models.Model(inp,out)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnAc1qFVjdHX"
      },
      "source": [
        "## Bigrams, trigrams and n-grams\n",
        "\n",
        "One limitation of the bag-of-words approach is that some words are part of multi-word expressions, for example, the word 'hot dog' has a completely different meaning from the words 'hot' and 'dog' in other contexts. If we represent the words 'hot' and 'dog' always using the same vectors, it can confuse our model.\n",
        "\n",
        "To address this, **n-gram representations** are often used in methods of document classification, where the frequency of each word, bi-word or tri-word is a useful feature for training classifiers. In bigram representations, for example, we will add all word pairs to the vocabulary, in addition to original words.\n",
        "\n",
        "Below is an example of how to generate a bigram bag of word representation using Scikit Learn:\n",
        "ngram_range: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.​\n",
        "\n",
        "token_pattern:Regular expression denoting what constitutes a “token”​\n",
        "\n",
        "Min_dif: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. ​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "vbseZkzBjdHY"
      },
      "outputs": [],
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)#  The specific regex r' b w+ b' matches a token with a word (w+) surrounded by spaces (b), i.e. the token must be separated from other tokens by spaces.\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "bigram_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
        "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCG_Gc8kjdHY"
      },
      "source": [
        "The main drawback of the n-gram approach is that the vocabulary size starts to grow extremely fast. In practice, we need to combine the n-gram representation with a dimensionality reduction technique, such as *embeddings*, which we will discuss in the next unit.\n",
        "\n",
        "To use an n-gram representation in our **AG News** dataset, we need to pass the `ngrams` parameter to our `TextVectorization` constructor. The length of a bigram vocaculary is **significantly larger**, in our case it is more than 1.3 million tokens! Thus it makes sense to limit bigram tokens as well by some reasonable number.\n",
        "\n",
        "We could use the same code as above to train the classifier, however, it would be very memory-inefficient. In the next unit, we will train the bigram classifier using embeddings. In the meantime, you can experiment with bigram classifier training in this notebook and see if you can get higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hm_0xS3jdHY"
      },
      "source": [
        "## Automatically calculating BoW Vectors\n",
        "\n",
        "In the example above we calculated BoW vectors by hand by summing the one-hot encodings of individual words. However, the latest version of TensorFlow allows us to calculate BoW vectors automatically by passing the `output_mode='count` parameter to the vectorizer constructor. This makes defining and training our model significanly easier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "Dh4QiZZvjdHY"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='count'),\n",
        "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
        "])\n",
        "print(\"Training vectorizer\")\n",
        "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "8Z7a4RzLjdHY"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkkFyIT_jdHY"
      },
      "source": [
        "## Term frequency - inverse document frequency (TF-IDF)\n",
        "\n",
        "In BoW representation, word occurrences are weighted using the same technique regardless of the word itself. However, it's clear that frequent words such as *a* and *in* are much less important for classification than specialized terms. In most NLP tasks some words are more relevant than others.\n",
        "\n",
        "**TF-IDF** stands for **term frequency - inverse document frequency**. It's a variation of bag-of-words, where instead of a binary 0/1 value indicating the appearance of a word in a document, a floating-point value is used, which is related to the frequency of the word occurrence in the corpus.\n",
        "\n",
        "More formally, the weight $w_{ij}$ of a word $i$ in the document $j$ is defined as:\n",
        "$$\n",
        "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
        "$$\n",
        "where\n",
        "* $tf_{ij}$ is the number of occurrences of $i$ in $j$, i.e. the BoW value we have seen before\n",
        "* $N$ is the number of documents in the collection\n",
        "* $df_i$ is the number of documents containing the word $i$ in the whole collection\n",
        "\n",
        "The TF-IDF value $w_{ij}$ increases proportionally to the number of times a word appears in a document and is offset by the number of documents in the corpus that contains the word, which helps to adjust for the fact that some words appear more frequently than others. For example, if the word appears in *every* document in the collection, $df_i=N$, and $w_{ij}=0$, and those terms would be completely disregarded.\n",
        "\n",
        "You can easily create TF-IDF vectorization of text using Scikit Learn:\n",
        "fit_transform in sklearn: Convert a collection of text documents to a matrix of token counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "D8wPVfrxjdHY"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['hot dogs']).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoZuM2TajdHY"
      },
      "source": [
        "In Keras, the `TextVectorization` layer can automatically compute TF-IDF frequencies by passing the `output_mode='tf-idf'` parameter. Let's repeat the code we used above to see if using TF-IDF increases accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "lSKiVl0_jdHY"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,output_mode='tf-idf'),\n",
        "    keras.layers.Dense(4,input_shape=(vocab_size,), activation='softmax')\n",
        "])\n",
        "print(\"Training vectorizer\")\n",
        "model.layers[0].adapt(ds_train.take(500).map(extract_text))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),validation_data=ds_test.map(tupelize).batch(batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "_6MzHcAfjdHY"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "yku2xER1jdHZ"
      },
      "outputs": [],
      "source": [
        "model.predict(['football'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "WWmi0RZxjdHZ"
      },
      "outputs": [],
      "source": [
        "texts = ['football basketball']\n",
        "predictions = model.predict(texts)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "id": "7HvlHicxjdHZ"
      },
      "outputs": [],
      "source": [
        "t = 0\n",
        "\n",
        "for text in texts:\n",
        "    i = 0\n",
        "    print(\"Prediction for \\\"%s\\\": \" % (text))\n",
        "    for label in classes:\n",
        "        print(\"\\t%s ==> %f\" % (label, predictions[t][i]))\n",
        "        i = i + 1\n",
        "    t = t + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prHTiU83jdHa"
      },
      "source": [
        "Try more sentences that relates to one of the classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-B_fKqRjdHa"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Even though TF-IDF representations provide frequency weights to different words, they are unable to represent meaning or order. As the famous linguist J. R. Firth said in 1935, \"The complete meaning of a word is always contextual, and no study of meaning apart from context can be taken seriously.\" We will learn how to capture contextual information from text using language modeling in a later unit."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oL9KopJirB2g"
      ],
      "name": "training_BoW_classifier.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}